# AI 发展简史

人工智能的故事，始于一群科学家在夏日会议上的狂想。1956年，达特茅斯学院，几位年轻的研究者聚在一起，给这个领域起了个响亮的名字——人工智能。当时的他们相信，只要把人类的思维规则写成代码，机器就能像人一样思考。这份自信支撑了整整三十年的探索。

## 符号主义的理想与现实

最初的路径看起来简单明了：让机器推理，就像人类在做逻辑题。图灵在1950年就抛出了那个著名的问题——如何判断机器是否具有智能？答案很直接：看它能不能在对话中骗过人类。这个测试后来成了AI研究的北极星，指引着无数后来者的方向。

那个年代的研究者们着迷于规则。他们把专家的知识拆解成一条条if-then语句，编织成复杂的逻辑网络。DENDRAL系统能够推断化学分子结构，MYCIN可以诊断血液感染——这些专家系统在特定领域展现出了惊人的准确率。医生们发现，MYCIN的诊断建议有时比他们自己的判断还要准确。

但现实很快暴露了这条路的局限。世界不是黑白分明的逻辑题。一个医生在诊断时，会综合病人的气色、语气、情绪，会考虑季节、地域、流行病趋势，这些模糊的、难以量化的因素，无法被硬编码进规则库。更致命的是，知识的获取和维护成本高得离谱。每添加一条新规则,可能就要修改十条旧规则来保持一致性。系统变得越来越笨重,最终不堪重负。

1980年代中后期，AI研究陷入了第一次寒冬。资金撤离，实验室关闭，"人工智能"这个词变成了笑话的代名词。

## 统计方法的悄然崛起

转机发生在另一个战场。当符号主义者们还在手工雕琢规则时，一批统计学家和工程师开始换个思路——既然写不出完美的规则，为什么不让机器自己从数据里找规律？

这个思路的数学根基早已存在。贝叶斯定理在18世纪就写下了它的公式：用先验概率和似然函数计算后验概率，用数据不断修正对世界的认知。只是当年缺乏算力和数据，这些优雅的公式只能躺在教科书里。互联网的普及改变了一切。搜索引擎记录下亿万次点击，电商平台沉淀了海量购买行为，这些数据成了新方法的养料。

支持向量机（SVM）在1990年代异军突起，统治了模式识别的各个角落———分类、回归、聚类，这些从数据中提取规律并做出判断的任务。它的核心思想简洁有力：在高维特征空间里找一个超平面，让不同类别的样本距离这个平面尽可能远。这个"最大间隔"原则背后是严格的数学推导——拉格朗日乘数法求解二次规划问题，对偶问题把计算复杂度从特征维度降到样本数量。更巧妙的是核函数的引入，它让算法能够在不显式计算高维坐标的情况下，直接度量样本间的相似度。一个径向基函数（RBF）核，就能把线性不可分的问题映射到无穷维空间里，找到完美的分割面。

这个时期的机器学习，高度依赖"特征工程"。研究者需要用领域知识手工设计特征——图像识别要提取SIFT特征点、HOG梯度方向，文本分类要设计TF-IDF权重、N-gram统计。这是个费时费力的活，但也正是人类智慧与机器算力结合的艺术。主成分分析（PCA）用线性代数中的特征值分解，把成百上千维的特征压缩到几十维的主轴上；奇异值分解（SVD）在协同过滤中大放异彩，从稀疏的评分矩阵里挖出隐藏的用户偏好和物品属性。

决策树用信息熵衡量分裂的价值，随机森林用集成学习对抗过拟合。贝叶斯分类假设特征独立，虽然这个假设在现实中常常不成立，但朴素贝叶斯依然在垃圾邮件过滤中表现出色——计算快、可解释、对高维稀疏数据不敏感。K-means聚类用欧氏距离划分族群，EM算法在隐变量空间里迭代优化，这些方法让AI第一次真正走进了普通人的生活。信用评分、商品推荐、欺诈检测，用户并不知道背后运转的是什么，但他们已经开始依赖这些算法的判断。

有趣的是，神经网络在这个时期并未消失，只是被压制在学术界的边缘。1986年，Rumelhart等人重新发现并推广了反向传播（Backpropagation）算法。它的核心是微积分的链式法则：从输出层的损失函数开始，逐层向前传播梯度，每一层的权重矩阵根据梯度下降更新参数。理论上，只要有足够多的隐藏层和神经元，神经网络可以逼近任意连续函数。这是通用逼近定理（Universal Approximation Theorem）赋予的理论保证。

但现实是残酷的。网络一加深，梯度在反向传播过程中不断衰减，传到前几层时已经接近于零——梯度消失让网络的前半段几乎学不到任何东西。激活函数的选择也是个难题，Sigmoid函数在饱和区的导数接近零，加剧了梯度消失；Tanh好一点，但本质问题依然存在。数据不够就过拟合，数据太多算力又跟不上，CPU逐个样本地计算前向传播和反向传播，训练一个网络可能要跑好几周。更要命的是，权重初始化稍有不慎，网络就陷入鞍点或局部极小值，再也醒不过来。

这些问题像三座大山压着神经网络。当SVM在各种竞赛中斩获冠军时，神经网络只能蜷缩在学术界的角落，等待命运的转机。少数坚持者——Geoffrey Hinton、Yann LeCun、Yoshua Bengio——在无人问津的领域里耕耘，他们相信，这条路早晚会走通。

## 深度学习的突然爆发

2012年的ImageNet竞赛，是AI历史上的分水岭。AlexNet横空出世，错误率比第二名低了十个百分点。这不是渐进式的改进，而是碾压式的胜利。秘密武器是三件事的完美组合：GPU的并行算力、ImageNet的百万级标注图片、以及八层深的卷积神经网络。

GPU的作用至关重要。它本是为图形渲染设计的硬件，数千个简单的计算核心可以并行执行矩阵运算。神经网络的前向传播和反向传播，本质上就是大量的矩阵乘法——输入特征矩阵乘以权重矩阵，逐层传递，反向时梯度矩阵也按相同的逻辑回传。NVIDIA的CUDA框架让这些运算可以高效映射到GPU上，训练速度比CPU快了几十倍。突然之间，那些曾经需要几周才能训练完的网络，几天就能跑出结果。

AlexNet引入了几个关键的技术突破。首先是ReLU激活函数，它的定义简单到令人发笑：`max(0, x)`。正值直接通过，负值截断为零。就这么简单的非线性变换，却解决了梯度消失的大问题——ReLU在正区间的梯度恒为1，不会衰减。其次是Dropout正则化，训练时随机丢弃一部分神经元，强迫网络学习更鲁棒的特征表示，防止过拟合。再加上数据增强（随机裁剪、翻转、颜色抖动），让有限的数据发挥出更大的价值。

这场胜利引爆了整个领域。卷积神经网络（CNN）的核心是卷积核——一个小的权重矩阵在图像上滑动，计算局部感受野的加权和。每个卷积核学习捕捉一种特定的模式：边缘、角点、纹理。池化层（Pooling）进行下采样，用最大值或平均值压缩空间维度，既减少计算量，又增强了位置不变性。这种"卷积-激活-池化"的层级结构，可以无限叠加。网络的浅层捕捉边缘和颜色，中层识别轮廓和纹理，深层则理解物体的语义——"这是一只猫"、"这是一辆车"。这种层次化的特征提取，恰好模拟了人类视觉皮层的工作方式。

VGGNet用更小的3×3卷积核堆叠得更深，证明了深度的价值。GoogLeNet引入Inception模块，并行使用多个尺度的卷积核，让网络自己学习该关注哪个尺度。ResNet在2015年把网络推到了152层，秘诀是残差连接（Residual Connection）：每隔几层加一个跳跃连接，让梯度可以直接绕过中间层传回去。这个简单的改动彻底解决了深层网络的训练难题，网络越深，效果反而越好。

循环神经网络（RNN）和它的改进版LSTM，则在序列数据上大放异彩。RNN的核心思想是维护一个隐藏状态，每个时间步接收输入，更新隐藏状态，再输出预测。理论上它可以记住任意长度的历史信息，但实际训练时，梯度在时间维度上的反向传播会爆炸或消失——展开的循环就像一个极深的网络，梯度要经过几十甚至上百次矩阵乘法。

LSTM（长短期记忆网络）用门控机制巧妙地解决了这个问题。它维护一个细胞状态（Cell State），像传送带一样贯穿整个序列。三个门控制信息流动：遗忘门决定丢弃哪些旧信息，输入门决定接纳哪些新信息，输出门决定暴露多少内部状态。这些门都是由Sigmoid函数输出0到1之间的权重，对信息进行加权过滤。通过这种设计，重要的信息可以在细胞状态里长期保存，梯度也能稳定地反向传播。语音识别的错误率从30%降到5%，机器翻译的BLEU分数逐年攀升，Siri和小冰能够进行基本的对话。时间终于成了神经网络可以建模的维度。

批归一化（Batch Normalization）是另一个里程碑式的技术。它在每一层的激活之前，对一批样本的特征进行标准化处理——减去均值，除以标准差。这样做稳定了每层输入的分布，缓解了"内部协变量偏移"问题，让网络可以用更大的学习率训练，收敛速度大幅提升。权重初始化的策略也越来越讲究，Xavier初始化和He初始化根据层的输入输出维度，精心设计方差，让激活值和梯度在各层之间保持合适的尺度。

2016年，AlphaGo战胜李世乭，把深度学习推向了公众视野的中心。围棋曾被认为是机器永远无法征服的领域——状态空间有 $10^{170}$ 种可能，远超宇宙中的原子数；一个局面好坏的评估，需要全局的判断和直觉，无法用简单的规则量化。AlphaGo用两个深度神经网络破解了这个难题：策略网络（Policy Network）预测人类高手会在哪里落子，价值网络（Value Network）评估当前局面的胜率。它们在人类棋谱上预训练，然后通过自我对弈不断强化。蒙特卡洛树搜索（MCTS）结合神经网络的评估，在虚拟的棋盘上探索数百万种可能，最终练就了超越人类的棋感。

这个时期的AI，开始展现出真正的"感知"能力。它能看懂图像，听懂语音，理解（有限的）文本。生成对抗网络（GAN）让机器学会了"创作"——生成器试图造假，判别器试图识破，两者对抗博弈，最终生成器能生成以假乱真的图像。StyleGAN可以生成逼真的人脸照片，这些人根本不存在；Pix2Pix能把草图转成照片，把黑白片上色。

但这些模型仍然是专用的。训练一个模型识别猫，它就只会识别猫；让它翻译英文，它就不能翻译法文。不同任务需要重新设计网络结构、重新标注数据、从头训练。知识无法跨任务迁移，每个模型都是一座孤岛。通用智能，似乎仍然遥不可及。研究者们开始意识到，问题的关键不在于网络的深度，而在于学习范式本身。

## 大模型时代的涌现

一切在2017年转向。Google的研究者发表了一篇论文，标题简洁而自信：《Attention Is All You Need》。Transformer架构诞生了，它将改写AI的未来。

这个架构的核心是自注意力机制（Self-Attention）。每个词在编码时，不再只看前面的词（像RNN那样），而是同时关注序列中所有其他词，计算它们之间的相关性。具体实现是三个矩阵变换：Query（查询）、Key（键）、Value（值）。每个词的表示向量分别乘以三个权重矩阵，得到 $Q$、$K$、$V$ 三个向量。然后用 $Q$ 去和所有词的 $K$ 做点积，算出注意力权重——两个词越相关，点积越大。这些权重经过Softmax归一化，用来加权求和所有的 $V$ 向量，得到最终的输出。整个过程可以并行计算，不需要像RNN那样一步步展开。

公式简洁而优雅：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

其中 $d_k$ 是向量维度，除以根号 $d_k$ 是为了稳定梯度。多头注意力（Multi-Head Attention）把这个机制并行重复多次，让模型从不同的子空间学习不同的语义关系。有的头关注语法结构，有的头捕捉语义相似，有的头识别长距离依赖。

起初，注意力机制只是众多技术改进中的一个小伎俩。它在2014年被引入机器翻译，帮助解码器在生成每个词时"瞄一眼"输入句子的不同位置。学界觉得这是个挺巧妙的补丁，但没人把它当回事——毕竟核心还是RNN，Attention只是个辅助。直到Google的团队激进地提出：干脆把RNN扔掉，只留下Attention，会怎么样？

结果让所有人震惊。Transformer抛弃了RNN的顺序结构，但序列的位置信息不能丢。位置编码（Positional Encoding）用正弦和余弦函数给每个位置分配一个独特的向量，加到词嵌入上，让模型知道每个词在句子中的位置。残差连接和层归一化（Layer Normalization）保证了深层网络的稳定训练。这些设计看似简单，组合起来却产生了魔法般的效果——训练速度比RNN快了十倍，可扩展性几乎没有上限。那个曾经不起眼的"小伎俩"，突然成了整座大厦的基石。

BERT（Bidirectional Encoder Representations from Transformers）在2018年横扫了NLP的各项任务榜单。它用Transformer的编码器部分，在海量文本上做自监督预训练：随机遮住15%的词，让模型预测这些被遮住的词是什么。这个简单的任务逼迫模型深度理解上下文——要预测"我[MASK]苹果"里的动词，模型必须理解"我"是主语、"苹果"是宾语，推断出动词应该是"吃"或"买"。预训练完成后，只需要在顶层加一个小的任务层，用少量标注数据微调，就能迁移到情感分析、命名实体识别、问答等各种任务。这种"预训练-微调"范式，第一次让知识可以跨任务复用。

与此同时，OpenAI选择了另一条路。GPT系列坚持Decoder-only架构，只用Transformer的解码器部分，专注于"预测下一个词"这个看似简单的任务。训练数据是互联网上的海量文本，模型从左到右逐个预测下一个词，最大化条件概率 $P(w_t | w_1, w_2, ..., w_{t-1})$。GPT-2有15亿参数，生成的文章已经相当流畅。GPT-3暴力扩展到1750亿参数，展现出了令人震惊的"少样本学习"能力——只需要在输入里给几个例子，模型就能理解任务意图，完成翻译、摘要、代码生成，甚至数学推理。

背后的秘密是Scaling Law（缩放定律）。OpenAI的研究者发现，模型的性能与参数量、数据量、计算量之间存在幂律关系。只要持续增加这三者，损失函数就会持续下降，能力就会持续提升。更神奇的是，当模型规模跨越某个临界点，一些能力会突然"涌现"出来——小模型完全不会的任务，大模型突然就会了。这种涌现不是线性增长，而是相变式的突破。

这个赌注在2022年兑现了。ChatGPT发布，全球震动。它不仅能流畅对话，还能写代码、改文章、编故事、做推理、扮演角色。更令人惊讶的是，这些能力并非针对性训练出来的，而是从语言建模这个单一任务中自然"涌现"的。当模型参数突破千亿量级，在数万亿token的文本上训练，某些质变悄然发生了——模型开始展现出常识推理、类比思维、甚至创造性。

秘密武器是RLHF（Reinforcement Learning from Human Feedback，人类反馈强化学习）。预训练后的模型虽然强大，但输出不可控——它可能生成有毒的、偏见的、无用的内容。OpenAI的方法是三步走：第一步，人类标注员对同一个问题的多个回答进行排序，训练一个奖励模型（Reward Model），让它学会判断什么是"好"的回答。第二步，用强化学习算法PPO（Proximal Policy Optimization）微调语言模型，让它最大化奖励模型的打分。第三步，用KL散度约束防止模型偏离原始预训练分布太远。在反复迭代中，模型学会了什么样的回答更符合人类的期待——不仅要准确，还要有用、安全、礼貌。这种对齐（Alignment）让AI第一次真正成为可用的助手，而不是实验室里的玩具。

类似的剧情在计算机视觉领域也重演了一遍。DALL-E、Midjourney、Stable Diffusion，这些文生图模型证明了，Transformer不仅能理解语言，也能理解视觉。它们用扩散模型（Diffusion Model）逐步从噪声中"去噪"出图像——训练时学习如何给干净图片加噪声，推理时反向运行这个过程，从纯噪声逐步还原出符合文本描述的图像。扩散过程本质上是个马尔可夫链，每一步都用一个神经网络（通常是U-Net）预测该减去多少噪声。Stable Diffusion在潜在空间（Latent Space）而非像素空间做扩散，大幅降低了计算成本，让个人电脑也能运行文生图模型。创作出的作品有时连人类艺术家都难辨真伪，AI绘画一度引发版权和伦理的激烈争论。

多模态大模型（Multimodal LLM）是下一步的方向。GPT-4V、Gemini、Claude能够同时处理文本、图像、语音，在不同模态之间自由切换。它们的训练数据包含图文配对、视频文本、音频转录，让模型学会跨模态的对齐和理解。给它一张菜谱照片，它能读出食材清单；给它一段代码截图，它能解释逻辑并找出bug；给它一个数学题的手写照片，它能理解题意并求解。模型内部的表示空间，开始统一不同模态的信息——视觉和语言不再是割裂的，而是在高维语义空间里交织融合。

思维链（Chain-of-Thought）提示技术让大模型的推理能力显现出来。只要在提示词里加上"让我们一步步思考"，模型就会展开中间推理过程，把复杂问题拆解成多个子步骤。这不是刻意训练的结果，而是模型从网上的解题过程、推理文本中自然学会的行为模式。配合工具使用能力（调用搜索引擎、计算器、代码解释器），大模型开始展现出某种"通用问题解决"的能力。

## 未完成的叙事

从1956年到2026年，七十年的时间里，AI走过了四个阶段：手工定义规则，统计数据规律，深度网络提取特征，大模型智能涌现。每一次转向，都在减少人为干预，增加机器的自主学习空间。但这条路径是否会继续延伸下去，没人能确定。

当前的大模型依然存在明显的缺陷。幻觉问题让它们偶尔一本正经地胡说八道，可解释性缺失让决策过程像个黑盒，训练成本高到只有少数机构能够承受，数据隐私和版权争议尚未有定论。更根本的问题是，这些模型是在做推理，还是仅仅在做统计意义上的模式匹配？当训练数据耗尽，Scaling Law还能继续吗？

符号主义的逻辑推理、统计方法的稳健性、深度学习的感知能力、大模型的涌现特性，这些曾经互相对抗的流派，可能需要在某个尚未出现的架构里重新整合。历史证明，被淘汰的方法往往会在新的土壤里复活——神经网络蛰伏了二十年后卷土重来，符号推理也许正在等待下一次机会。

AI的历史是一部关于"智能"定义的变迁史。每次技术突破之后，"智能"的标准就会被重新划定。下棋、识图、对话，这些曾经被认为需要智能才能完成的任务，一旦被机器攻克，就不再被视为智能的核心。真正的智能，似乎永远在下一个未被攻克的堡垒里。
